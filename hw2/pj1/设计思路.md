### 1. **基础准备**
   - **环境理解**：CartPole是一个物理模拟任务，代理控制小车左右移动（2个动作），目标是让杆子保持直立（状态包括位置、速度、角度、角速度4维连续值）。每步存活得+1奖励，失败（杆子倒或超出边界）结束。
   - **设计思路**：创建Q表（一个多维数组）存储状态-动作的价值。超参数如学习率（0.1，控制更新速度）、折扣因子（0.99，重视未来奖励）、探索率ε（初始1.0，逐渐衰减到0.01）用于平衡学习稳定性和探索。状态需离散化（分成20x20x40x40个“桶”），因为Q-Learning适合有限状态，避免连续空间的无限可能。边界扩展（如位置±2.5）确保覆盖实际范围，防止信息丢失。

### 2. **核心学习循环**
   - **动作选择**：用ε-greedy策略——随机数<ε时随机探索（试新动作），否则选Q值最高的动作（利用已知知识）。这像“先乱试，后精炼”，初期多探索（ε高），后期多利用（ε低，衰减率0.999确保缓慢）。
   - **状态更新与学习**：每步执行动作，观察新状态和奖励。用Bellman方程更新Q值：当前Q = 当前Q + 学习率 × (奖励 + 折扣 × 下一状态最佳Q - 当前Q)。这逐步优化Q表，让代理预测长期回报。
   - **循环设计**：运行10000个episode（每episode从重置开始，到失败结束），记录总奖励。每100 episode打印平均奖励，监控进步。增加episode数确保充分采样，解决状态空间大时的收敛慢问题。

### 3. **验证效果**
   - **测试思路**：无探索，只用Q表选最优动作，运行100个episode。计算平均奖励和成功率（奖励≥195视为成功，接近环境上限200步）。
   - **可视化**：绘制奖励曲线图并保存为PNG文件，便于观察学习趋势（奖励从低到高表示改进）。

